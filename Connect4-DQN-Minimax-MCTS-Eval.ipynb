{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta16A_T1fzFg",
    "outputId": "de787c24-cb2a-4576-b493-b3ecc310e7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 19:55:44.147072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/titus/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import dqn\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "from open_spiel.python.algorithms import minimax\n",
    "from open_spiel.python.algorithms import minimax_agent\n",
    "from open_spiel.python.algorithms import mcts\n",
    "from open_spiel.python.algorithms import mcts_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Lm5xaFzxgSw6"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "#Directory to save/load the agent models.\n",
    "checkpoint_dir = \"/Users/titus/Desktop/ml/trainingResults/connectfour\"\n",
    "\n",
    "#Episode frequency at which the DQN agent models are saved\n",
    "save_every = int(1e4)\n",
    "\n",
    "#Number of training episodes.\n",
    "num_train_episodes = int(1e6)\n",
    "\n",
    "#Episode frequency at which the DQN agents are evaluated.\n",
    "eval_every = 100\n",
    "\n",
    "\n",
    "# DQN model hyper-parameters\n",
    "\n",
    "#Number of hidden units in the Q-Network MLP\n",
    "hidden_layers_sizes = [64, 64]\n",
    "\n",
    "#Size of the replay buffer\n",
    "replay_buffer_capacity = int(1e5)\n",
    "\n",
    "#Number of transitions to sample at each learning step.                     \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_board(s):\n",
    "    # Split the string representation into rows and columns\n",
    "    rows = s.strip().split('\\n')\n",
    "    # Create a 2D array (7x6) to represent the board\n",
    "    board = np.zeros((6, 7), dtype=int)\n",
    "    for i, row in enumerate(rows):\n",
    "        for j, val in enumerate(row):\n",
    "            if val == 'x':\n",
    "                board[5 - i][j] = 1  # Assuming 'x' represents Player 1\n",
    "            elif val == 'o':\n",
    "                board[5 - i][j] = 2  # Assuming 'o' represents Player 2\n",
    "    return board\n",
    "\n",
    "def evaluate(state):\n",
    "    board = string_to_board(state.observation_string())\n",
    "    # Define evaluation weights for different patterns\n",
    "    weights = [0, 1, 10, 100, 1000]\n",
    "\n",
    "    def score_window(window, player):\n",
    "        opp_player = 1 if player == 2 else 2\n",
    "        if window.count(player) == 4:\n",
    "            return weights[4]\n",
    "        elif window.count(player) == 3 and window.count(0) == 1:\n",
    "            return weights[3]\n",
    "        elif window.count(player) == 2 and window.count(0) == 2:\n",
    "            return weights[2]\n",
    "        elif window.count(opp_player) == 3 and window.count(0) == 1:\n",
    "            return -weights[3]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    rows, cols = board.shape\n",
    "    score = 0\n",
    "\n",
    "    # Evaluate rows\n",
    "    for r in range(rows):\n",
    "        for c in range(cols - 3):\n",
    "            window = list(board[r, c:c + 4])\n",
    "            score += score_window(window, 1) + score_window(window, 2)\n",
    "\n",
    "    # Evaluate columns\n",
    "    for c in range(cols):\n",
    "        for r in range(rows - 3):\n",
    "            window = list(board[r:r + 4, c])\n",
    "            score += score_window(window, 1) + score_window(window, 2)\n",
    "\n",
    "    # Evaluate diagonals\n",
    "    for r in range(rows - 3):\n",
    "        for c in range(cols - 3):\n",
    "            window = list(board[r:r + 4, c:c + 4].diagonal())\n",
    "            score += score_window(window, 1) + score_window(window, 2)\n",
    "\n",
    "            window = list(np.fliplr(board[r:r + 4, c:c + 4]).diagonal())\n",
    "            score += score_window(window, 1) + score_window(window, 2)\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 19:57:14.696511: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/ml/trainingResults/connectfour/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/ml/trainingResults/connectfour/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/ml/trainingResults/connectfour/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/ml/trainingResults/connectfour/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/ml/trainingResults/connectfour/q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/ml/trainingResults/connectfour/q_network_pid1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/ml/trainingResults/connectfour/target_q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/ml/trainingResults/connectfour/target_q_network_pid1\n"
     ]
    }
   ],
   "source": [
    "game = \"connect_four\"\n",
    "num_players = 2\n",
    "env = rl_environment.Environment(game, include_full_state=True)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "#random agents for evaluation\n",
    "minimax_agents = [\n",
    "    minimax_agent.MiniMaxAgent(env.game, env.get_state, player_id=idx, num_actions=num_actions, maximum_depth=8, value_function=evaluate)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "mcts_bot = mcts.MCTSBot(env.game, 1.5, 100, mcts.RandomRolloutEvaluator())\n",
    "mcts_agents = [\n",
    "        mcts_agent.MCTSAgent(player_id=idx, num_actions=num_actions, mcts_bot=mcts_bot)\n",
    "        for idx in range(num_players)\n",
    "]\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    agents = [\n",
    "          dqn.DQN(\n",
    "              session=sess,\n",
    "              player_id=idx,\n",
    "              state_representation_size=info_state_size,\n",
    "              num_actions=num_actions,\n",
    "              hidden_layers_sizes=hidden_layers_sizes,\n",
    "              replay_buffer_capacity=replay_buffer_capacity,\n",
    "              batch_size=batch_size) for idx in range(num_players)\n",
    "      ]\n",
    "    for agent in agents:\n",
    "        agent.restore(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_mcts(env, trained_agents, mcts_agents, num_episodes):\n",
    "  \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  results = np.zeros((num_players, 3), dtype=int)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = mcts_agents[:]    \n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    for ep in range(num_episodes):\n",
    "      time_step = env.reset()\n",
    "      episode_rewards = 0\n",
    "      while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "        action_list = [agent_output.action]\n",
    "        time_step = env.step(action_list)\n",
    "        print(time_step.observation_string())\n",
    "        episode_rewards += time_step.rewards[player_pos]\n",
    "      results[player_pos, int(episode_rewards % 3)] += 1\n",
    "      sum_episode_rewards[player_pos] += episode_rewards\n",
    "      if (ep + 1) % 10 == 0:\n",
    "          print(f\"Episode {ep+1}\")\n",
    "          results_percentage = (results * 100).astype(float) / (ep + 1)\n",
    "          results_percentage_str =  np.array([\"{:.2f}%\".format(x) for x in results_percentage.flatten()]).reshape(results_percentage.shape)\n",
    "          for pid in range(num_players):\n",
    "              print(f\"Results for DQN Agent {pid}\")\n",
    "              print(f\"   Win:  {results[pid, 1]}   ({results_percentage_str[pid, 1]})\")\n",
    "              print(f\"   Draw: {results[pid, 0]}   ({results_percentage_str[pid, 0]})\") \n",
    "              print(f\"   Lost: {results[pid, 2]}   ({results_percentage_str[pid, 2]})\")\n",
    "  return sum_episode_rewards / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TimeStep' object has no attribute 'observation_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_against_mcts\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcts_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m, in \u001b[0;36meval_against_mcts\u001b[0;34m(env, trained_agents, mcts_agents, num_episodes)\u001b[0m\n\u001b[1;32m     15\u001b[0m   action_list \u001b[38;5;241m=\u001b[39m [agent_output\u001b[38;5;241m.\u001b[39maction]\n\u001b[1;32m     16\u001b[0m   time_step \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action_list)\n\u001b[0;32m---> 17\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[43mtime_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_string\u001b[49m())\n\u001b[1;32m     18\u001b[0m   episode_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m time_step\u001b[38;5;241m.\u001b[39mrewards[player_pos]\n\u001b[1;32m     19\u001b[0m results[player_pos, \u001b[38;5;28mint\u001b[39m(episode_rewards \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m3\u001b[39m)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TimeStep' object has no attribute 'observation_string'"
     ]
    }
   ],
   "source": [
    "eval_against_mcts(env, agents, mcts_agents, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_minimax(env, trained_agents, minimax_agents, value_function, maximum_depth):\n",
    "  \"\"\"Evaluates `trained_agents` against `minimax` for.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = minimax_agents[:]\n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    time_step = env.reset()\n",
    "    episode_rewards = 0\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      if player_id == player_pos:\n",
    "        agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "        action_list = [agent_output.action]\n",
    "      else:\n",
    "        _ , action = minimax.alpha_beta_search(env.game, env.get_state, value_function=value_function, maximum_depth = maximum_depth,maximizing_player_id=player_id)\n",
    "        action_list = [action]\n",
    "      time_step = env.step(action_list)\n",
    "    sum_episode_rewards[player_pos] = time_step.rewards[player_pos]    \n",
    "  return sum_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax with Depth of:  1  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  2  Reward of DQN Agents:  [-1.  1.]\n",
      "Minimax with Depth of:  3  Reward of DQN Agents:  [-1. -1.]\n",
      "Minimax with Depth of:  4  Reward of DQN Agents:  [-1. -1.]\n",
      "Minimax with Depth of:  5  Reward of DQN Agents:  [-1.  1.]\n",
      "Minimax with Depth of:  6  Reward of DQN Agents:  [-1. -1.]\n",
      "Minimax with Depth of:  7  Reward of DQN Agents:  [-1.  1.]\n",
      "Minimax with Depth of:  8  Reward of DQN Agents:  [-1. -1.]\n",
      "Minimax with Depth of:  9  Reward of DQN Agents:  [-1. -1.]\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    agents2 = [agents[1], agents[0]]\n",
    "    # 0 for tie, 1 for win, -1 for lost\n",
    "    for i in range(1,10):\n",
    "        print(\"Minimax with Depth of: \", i , \" Reward of DQN Agents: \", eval_against_minimax(env, agents, minimax_agents, evaluate, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_minimax_mcts(env, trained_agents, minimax_agents, value_function, maximum_depth, num_episodes):\n",
    "  \"\"\"Evaluates `trained_agents` against `minimax` for.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = minimax_agents[:]\n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    for ep in range(num_episodes):\n",
    "        time_step = env.reset()\n",
    "        episode_rewards = 0\n",
    "        while not time_step.last():\n",
    "          player_id = time_step.observations[\"current_player\"]\n",
    "          if player_id == player_pos:\n",
    "            agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "            action_list = [agent_output.action]\n",
    "          else:\n",
    "            _ , action = minimax.alpha_beta_search(env.game, env.get_state, value_function=value_function, maximum_depth = maximum_depth,maximizing_player_id=player_id)\n",
    "            action_list = [action]\n",
    "          time_step = env.step(action_list)\n",
    "          episode_rewards += time_step.rewards[player_pos]\n",
    "        sum_episode_rewards[player_pos] += episode_rewards    \n",
    "  return sum_episode_rewards / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax with Depth of:  1  Reward of MCTS Agents:  [1. 1.]\n",
      "Minimax with Depth of:  2  Reward of MCTS Agents:  [0.85 0.87]\n",
      "Minimax with Depth of:  3  Reward of MCTS Agents:  [0.8  0.88]\n",
      "Minimax with Depth of:  4  Reward of MCTS Agents:  [0.74 0.22]\n",
      "Minimax with Depth of:  5  Reward of MCTS Agents:  [0.35 0.13]\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    agents2 = [agents[1], agents[0]]\n",
    "    # 0 for tie, 1 for win, -1 for lost\n",
    "    for i in range(1,9):\n",
    "        print(\"Minimax with Depth of: \", i , \" Reward of MCTS Agents: \", eval_minimax_mcts(env, mcts_agents, minimax_agents, evaluate, i, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
