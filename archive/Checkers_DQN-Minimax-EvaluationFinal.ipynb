{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta16A_T1fzFg",
    "outputId": "de787c24-cb2a-4576-b493-b3ecc310e7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 18:57:05.317679: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/titus/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import dqn\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "from open_spiel.python.algorithms import minimax\n",
    "from open_spiel.python.algorithms import minimax_agent\n",
    "from open_spiel.python.algorithms import mcts\n",
    "from open_spiel.python.algorithms import mcts_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Lm5xaFzxgSw6"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "#Directory to save/load the agent models.\n",
    "checkpoint_dir = \"/Users/titus/Desktop/MLAgents/Checkers2\"\n",
    "\n",
    "#Episode frequency at which the DQN agent models are saved\n",
    "save_every = int(1e4)\n",
    "\n",
    "#Number of training episodes.\n",
    "num_train_episodes = int(1e6)\n",
    "\n",
    "#Episode frequency at which the DQN agents are evaluated.\n",
    "eval_every = 100\n",
    "\n",
    "\n",
    "# DQN model hyper-parameters\n",
    "\n",
    "#Number of hidden units in the Q-Network MLP\n",
    "hidden_layers_sizes = [64, 64]\n",
    "\n",
    "#Size of the replay buffer\n",
    "replay_buffer_capacity = int(1e5)\n",
    "\n",
    "#Number of transitions to sample at each learning step.                     \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(state):\n",
    "  kWhite = state.observation_string().count('o')\n",
    "  kWhiteKing = state.observation_string().count('8') - 1 #one row is also labeld 8\n",
    "  kBlack = state.observation_string().count('+') \n",
    "  kBlackKing = state.observation_string().count('*')\n",
    "  return -1 * state.current_player() * (kWhite + 2*kWhiteKing - kBlack - 2*kBlackKing) #player 0 moves white pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 18:57:10.011259: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid1\n"
     ]
    }
   ],
   "source": [
    "game = \"checkers\"\n",
    "num_players = 2\n",
    "env = rl_environment.Environment(game, include_full_state=True)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "#random agents for evaluation\n",
    "minimax_agents = [\n",
    "    minimax_agent.MiniMaxAgent(env.game, env.get_state, player_id=idx, num_actions=num_actions, maximum_depth=8, value_function=evaluate)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "mcts_bot = mcts.MCTSBot(env.game, 1.5, 100, mcts.RandomRolloutEvaluator())\n",
    "mcts_agents = [\n",
    "        mcts_agent.MCTSAgent(player_id=idx, num_actions=num_actions, mcts_bot=mcts_bot)\n",
    "        for idx in range(num_players)\n",
    "]\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    agents = [\n",
    "          dqn.DQN(\n",
    "              session=sess,\n",
    "              player_id=idx,\n",
    "              state_representation_size=info_state_size,\n",
    "              num_actions=num_actions,\n",
    "              hidden_layers_sizes=hidden_layers_sizes,\n",
    "              replay_buffer_capacity=replay_buffer_capacity,\n",
    "              batch_size=batch_size) for idx in range(num_players)\n",
    "      ]\n",
    "    for agent in agents:\n",
    "        agent.restore(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_mcts(env, trained_agents, mcts_agents, num_episodes):\n",
    "  \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  results = np.zeros((num_players, 3), dtype=int)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = mcts_agents[:]    \n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    for ep in range(num_episodes):\n",
    "      time_step = env.reset()\n",
    "      episode_rewards = 0\n",
    "      while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "        action_list = [agent_output.action]\n",
    "        time_step = env.step(action_list)\n",
    "        episode_rewards += time_step.rewards[player_pos]\n",
    "      results[player_pos, int(episode_rewards % 3)] += 1\n",
    "      sum_episode_rewards[player_pos] += episode_rewards\n",
    "      if (ep + 1) % 10 == 0:\n",
    "          print(f\"Episode {ep+1}\")\n",
    "          results_percentage = (results * 100).astype(float) / (ep + 1)\n",
    "          results_percentage_str =  np.array([\"{:.2f}%\".format(x) for x in results_percentage.flatten()]).reshape(results_percentage.shape)\n",
    "          for pid in range(num_players):\n",
    "              print(f\"Results for DQN Agent {pid}\")\n",
    "              print(f\"   Win:  {results[pid, 1]}   ({results_percentage_str[pid, 1]})\")\n",
    "              print(f\"   Draw: {results[pid, 0]}   ({results_percentage_str[pid, 0]})\") \n",
    "              print(f\"   Lost: {results[pid, 2]}   ({results_percentage_str[pid, 2]})\")\n",
    "  return sum_episode_rewards / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_against_mcts\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcts_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36meval_against_mcts\u001b[0;34m(env, trained_agents, mcts_agents, num_episodes)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m time_step\u001b[38;5;241m.\u001b[39mlast():\n\u001b[1;32m     13\u001b[0m   player_id \u001b[38;5;241m=\u001b[39m time_step\u001b[38;5;241m.\u001b[39mobservations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_player\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m   agent_output \u001b[38;5;241m=\u001b[39m \u001b[43mcur_agents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplayer_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_evaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m   action_list \u001b[38;5;241m=\u001b[39m [agent_output\u001b[38;5;241m.\u001b[39maction]\n\u001b[1;32m     16\u001b[0m   time_step \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action_list)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/mcts_agent.py:46\u001b[0m, in \u001b[0;36mMCTSAgent.step\u001b[0;34m(self, time_step, is_evaluation)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Call the MCTS bot's step to get the action.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_actions)\n\u001b[0;32m---> 46\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mcts_bot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m probs[action] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rl_agent\u001b[38;5;241m.\u001b[39mStepOutput(action\u001b[38;5;241m=\u001b[39maction, probs\u001b[38;5;241m=\u001b[39mprobs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/mcts.py:290\u001b[0m, in \u001b[0;36mMCTSBot.step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m--> 290\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_with_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/mcts.py:264\u001b[0m, in \u001b[0;36mMCTSBot.step_with_policy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns bot's policy and action at given state.\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 264\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m best \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39mbest_child()\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/mcts.py:406\u001b[0m, in \u001b[0;36mMCTSBot.mcts_search\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    404\u001b[0m   solved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolve\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m   returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworking_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m   solved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m visit_path:\n\u001b[1;32m    410\u001b[0m   \u001b[38;5;66;03m# For chance nodes, walk up the tree to find the decision-maker.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/mcts.py:66\u001b[0m, in \u001b[0;36mRandomRolloutEvaluator.evaluate\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state\u001b[38;5;241m.\u001b[39mchoice(working_state\u001b[38;5;241m.\u001b[39mlegal_actions())\n\u001b[0;32m---> 66\u001b[0m   \u001b[43mworking_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m returns \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(working_state\u001b[38;5;241m.\u001b[39mreturns())\n\u001b[1;32m     68\u001b[0m result \u001b[38;5;241m=\u001b[39m returns \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m result \u001b[38;5;241m+\u001b[39m returns\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_against_mcts(env, agents, mcts_agents, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_minimax(env, trained_agents, minimax_agents, value_function, maximum_depth):\n",
    "  \"\"\"Evaluates `trained_agents` against `minimax` for.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = minimax_agents[:]\n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      if player_id == player_pos:\n",
    "        agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "        action_list = [agent_output.action]\n",
    "      else:\n",
    "        _ , action = minimax.alpha_beta_search(env.game, env.get_state, value_function=value_function, maximum_depth = maximum_depth,maximizing_player_id=player_id)\n",
    "        action_list = [action]\n",
    "      time_step = env.step(action_list)\n",
    "      #print(time_step.rewards[player_pos])\n",
    "    sum_episode_rewards[player_pos] = time_step.rewards[player_pos]    \n",
    "  return sum_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Minimax with Depth of:  1  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  2  Reward of DQN Agents:  [0. 1.]\n",
      "Minimax with Depth of:  3  Reward of DQN Agents:  [0. 1.]\n",
      "Minimax with Depth of:  4  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  5  Reward of DQN Agents:  [1. 0.]\n",
      "Minimax with Depth of:  6  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  7  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  8  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  9  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  10  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  11  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  12  Reward of DQN Agents:  [1. 1.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 0 for tie, 1 for win, -1 for lost\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinimax with Depth of: \u001b[39m\u001b[38;5;124m\"\u001b[39m, i , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Reward of DQN Agents: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43meval_against_minimax\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminimax_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36meval_against_minimax\u001b[0;34m(env, trained_agents, minimax_agents, value_function, maximum_depth)\u001b[0m\n\u001b[1;32m     13\u001b[0m   action_list \u001b[38;5;241m=\u001b[39m [agent_output\u001b[38;5;241m.\u001b[39maction]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m   _ , action \u001b[38;5;241m=\u001b[39m \u001b[43mminimax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_beta_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximum_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmaximum_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmaximizing_player_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplayer_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m   action_list \u001b[38;5;241m=\u001b[39m [action]\n\u001b[1;32m     17\u001b[0m time_step \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action_list)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/minimax.py:143\u001b[0m, in \u001b[0;36malpha_beta_search\u001b[0;34m(game, state, value_function, maximum_depth, maximizing_player_id)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maximizing_player_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m   maximizing_player_id \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mcurrent_player()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_alpha_beta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximum_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximizing_player_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximizing_player_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/minimax.py:71\u001b[0m, in \u001b[0;36m_alpha_beta\u001b[0;34m(state, depth, alpha, beta, value_function, maximizing_player_id)\u001b[0m\n\u001b[1;32m     69\u001b[0m child_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     70\u001b[0m child_state\u001b[38;5;241m.\u001b[39mapply_action(action)\n\u001b[0;32m---> 71\u001b[0m child_value, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_alpha_beta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximizing_player_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child_value \u001b[38;5;241m>\u001b[39m value:\n\u001b[1;32m     74\u001b[0m   value \u001b[38;5;241m=\u001b[39m child_value\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/minimax.py:85\u001b[0m, in \u001b[0;36m_alpha_beta\u001b[0;34m(state, depth, alpha, beta, value_function, maximizing_player_id)\u001b[0m\n\u001b[1;32m     83\u001b[0m child_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     84\u001b[0m child_state\u001b[38;5;241m.\u001b[39mapply_action(action)\n\u001b[0;32m---> 85\u001b[0m child_value, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_alpha_beta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximizing_player_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child_value \u001b[38;5;241m<\u001b[39m value:\n\u001b[1;32m     88\u001b[0m   value \u001b[38;5;241m=\u001b[39m child_value\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/minimax.py:71\u001b[0m, in \u001b[0;36m_alpha_beta\u001b[0;34m(state, depth, alpha, beta, value_function, maximizing_player_id)\u001b[0m\n\u001b[1;32m     69\u001b[0m child_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     70\u001b[0m child_state\u001b[38;5;241m.\u001b[39mapply_action(action)\n\u001b[0;32m---> 71\u001b[0m child_value, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_alpha_beta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximizing_player_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child_value \u001b[38;5;241m>\u001b[39m value:\n\u001b[1;32m     74\u001b[0m   value \u001b[38;5;241m=\u001b[39m child_value\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/minimax.py:85\u001b[0m, in \u001b[0;36m_alpha_beta\u001b[0;34m(state, depth, alpha, beta, value_function, maximizing_player_id)\u001b[0m\n\u001b[1;32m     83\u001b[0m child_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     84\u001b[0m child_state\u001b[38;5;241m.\u001b[39mapply_action(action)\n\u001b[0;32m---> 85\u001b[0m child_value, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_alpha_beta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximizing_player_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child_value \u001b[38;5;241m<\u001b[39m value:\n\u001b[1;32m     88\u001b[0m   value \u001b[38;5;241m=\u001b[39m child_value\n",
      "    \u001b[0;31m[... skipping similar frames: _alpha_beta at line 71 (4 times), _alpha_beta at line 85 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/minimax.py:85\u001b[0m, in \u001b[0;36m_alpha_beta\u001b[0;34m(state, depth, alpha, beta, value_function, maximizing_player_id)\u001b[0m\n\u001b[1;32m     83\u001b[0m child_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     84\u001b[0m child_state\u001b[38;5;241m.\u001b[39mapply_action(action)\n\u001b[0;32m---> 85\u001b[0m child_value, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_alpha_beta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximizing_player_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child_value \u001b[38;5;241m<\u001b[39m value:\n\u001b[1;32m     88\u001b[0m   value \u001b[38;5;241m=\u001b[39m child_value\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/minimax.py:71\u001b[0m, in \u001b[0;36m_alpha_beta\u001b[0;34m(state, depth, alpha, beta, value_function, maximizing_player_id)\u001b[0m\n\u001b[1;32m     69\u001b[0m child_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     70\u001b[0m child_state\u001b[38;5;241m.\u001b[39mapply_action(action)\n\u001b[0;32m---> 71\u001b[0m child_value, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_alpha_beta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalue_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximizing_player_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m child_value \u001b[38;5;241m>\u001b[39m value:\n\u001b[1;32m     74\u001b[0m   value \u001b[38;5;241m=\u001b[39m child_value\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/open_spiel/python/algorithms/minimax.py:62\u001b[0m, in \u001b[0;36m_alpha_beta\u001b[0;34m(state, depth, alpha, beta, value_function, maximizing_player_id)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     59\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe assume we can walk the full depth of the tree. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry increasing the maximum_depth or provide a value_function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m player \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mcurrent_player()\n\u001b[1;32m     65\u001b[0m best_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      3\u001b[0m kWhiteKing \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mobservation_string()\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m#one row is also labeld 8\u001b[39;00m\n\u001b[1;32m      4\u001b[0m kBlack \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mobservation_string()\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m----> 5\u001b[0m kBlackKing \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m state\u001b[38;5;241m.\u001b[39mcurrent_player() \u001b[38;5;241m*\u001b[39m (kWhite \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mkWhiteKing \u001b[38;5;241m-\u001b[39m kBlack \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mkBlackKing)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " with sess.as_default():\n",
    "    agents2 = [agents[1], agents[0]]\n",
    "    print(type(agents))\n",
    "    # 0 for tie, 1 for win, -1 for lost\n",
    "    for i in range(1,20):\n",
    "        print(\"Minimax with Depth of: \", i , \" Reward of DQN Agents: \", eval_against_minimax(env, agents, minimax_agents, evaluate, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
