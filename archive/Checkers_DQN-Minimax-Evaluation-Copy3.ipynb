{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta16A_T1fzFg",
    "outputId": "de787c24-cb2a-4576-b493-b3ecc310e7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 18:12:13.230228: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/titus/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import dqn\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "from open_spiel.python.algorithms import minimax\n",
    "from open_spiel.python.algorithms import minimax_agent\n",
    "from open_spiel.python.algorithms import mcts\n",
    "from open_spiel.python.algorithms import mcts_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Lm5xaFzxgSw6"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "#Directory to save/load the agent models.\n",
    "checkpoint_dir = \"/Users/titus/Desktop/MLAgents/Checkers2\"\n",
    "\n",
    "#Episode frequency at which the DQN agent models are saved\n",
    "save_every = int(1e4)\n",
    "\n",
    "#Number of training episodes.\n",
    "num_train_episodes = int(1e6)\n",
    "\n",
    "#Episode frequency at which the DQN agents are evaluated.\n",
    "eval_every = 100\n",
    "\n",
    "\n",
    "# DQN model hyper-parameters\n",
    "\n",
    "#Number of hidden units in the Q-Network MLP\n",
    "hidden_layers_sizes = [64, 64]\n",
    "\n",
    "#Size of the replay buffer\n",
    "replay_buffer_capacity = int(1e5)\n",
    "\n",
    "#Number of transitions to sample at each learning step.                     \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(state):\n",
    "  kWhite = state.observation_string().count('o')\n",
    "  kWhiteKing = state.observation_string().count('8') - 1 #one row is also labeld 8\n",
    "  kBlack = state.observation_string().count('+') \n",
    "  kBlackKing = state.observation_string().count('*')\n",
    "  return -1 * state.current_player() * (kWhite + 2*kWhiteKing - kBlack - 2*kBlackKing) #player 0 moves white pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 18:12:17.433417: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid1\n"
     ]
    }
   ],
   "source": [
    "game = \"checkers\"\n",
    "num_players = 2\n",
    "env = rl_environment.Environment(game, include_full_state=True)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "#random agents for evaluation\n",
    "minimax_agents = [\n",
    "    minimax_agent.MiniMaxAgent(env.game, env.get_state, player_id=idx, num_actions=num_actions, maximum_depth=8, value_function=evaluate)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "mcts_bot = mcts.MCTSBot(env.game, 1.5, 100, mcts.RandomRolloutEvaluator())\n",
    "mcts_agents = [\n",
    "        mcts_agent.MCTSAgent(player_id=idx, num_actions=num_actions, mcts_bot=mcts_bot)\n",
    "        for idx in range(num_players)\n",
    "]\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    agents = [\n",
    "          dqn.DQN(\n",
    "              session=sess,\n",
    "              player_id=idx,\n",
    "              state_representation_size=info_state_size,\n",
    "              num_actions=num_actions,\n",
    "              hidden_layers_sizes=hidden_layers_sizes,\n",
    "              replay_buffer_capacity=replay_buffer_capacity,\n",
    "              batch_size=batch_size) for idx in range(num_players)\n",
    "      ]\n",
    "    for agent in agents:\n",
    "        agent.restore(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_mcts(env, trained_agents, mcts_agents, num_episodes):\n",
    "  \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  results = np.zeros((num_players, 3), dtype=int)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = mcts_agents[:]    \n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    for ep in range(num_episodes):\n",
    "      time_step = env.reset()\n",
    "      episode_rewards = 0\n",
    "      while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "        action_list = [agent_output.action]\n",
    "        time_step = env.step(action_list)\n",
    "        episode_rewards += time_step.rewards[player_pos]\n",
    "      results[player_pos, int(episode_rewards % 3)] += 1\n",
    "      sum_episode_rewards[player_pos] += episode_rewards\n",
    "      if (ep + 1) % 10 == 0:\n",
    "          print(f\"Episode {ep+1}\")\n",
    "          results_percentage = (results * 100).astype(float) / (ep + 1)\n",
    "          results_percentage_str =  np.array([\"{:.2f}%\".format(x) for x in results_percentage.flatten()]).reshape(results_percentage.shape)\n",
    "          for pid in range(num_players):\n",
    "              print(f\"Results for DQN Agent {pid}\")\n",
    "              print(f\"   Win:  {results[pid, 1]}   ({results_percentage_str[pid, 1]})\")\n",
    "              print(f\"   Draw: {results[pid, 0]}   ({results_percentage_str[pid, 0]})\") \n",
    "              print(f\"   Lost: {results[pid, 2]}   ({results_percentage_str[pid, 2]})\")\n",
    "  return sum_episode_rewards / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\n",
      "Results for DQN Agent 0\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 2   (20.00%)\n",
      "   Lost: 8   (80.00%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 20\n",
      "Results for DQN Agent 0\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 4   (20.00%)\n",
      "   Lost: 16   (80.00%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 30\n",
      "Results for DQN Agent 0\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 8   (26.67%)\n",
      "   Lost: 22   (73.33%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 40\n",
      "Results for DQN Agent 0\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 9   (22.50%)\n",
      "   Lost: 31   (77.50%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 50\n",
      "Results for DQN Agent 0\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 9   (18.00%)\n",
      "   Lost: 41   (82.00%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 60\n",
      "Results for DQN Agent 0\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 12   (20.00%)\n",
      "   Lost: 48   (80.00%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 70\n",
      "Results for DQN Agent 0\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 13   (18.57%)\n",
      "   Lost: 57   (81.43%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 80\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (1.25%)\n",
      "   Draw: 15   (18.75%)\n",
      "   Lost: 64   (80.00%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 90\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (1.11%)\n",
      "   Draw: 15   (16.67%)\n",
      "   Lost: 74   (82.22%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 100\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (1.00%)\n",
      "   Draw: 18   (18.00%)\n",
      "   Lost: 81   (81.00%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 110\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (0.91%)\n",
      "   Draw: 20   (18.18%)\n",
      "   Lost: 89   (80.91%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 120\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (0.83%)\n",
      "   Draw: 20   (16.67%)\n",
      "   Lost: 99   (82.50%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 130\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (0.77%)\n",
      "   Draw: 21   (16.15%)\n",
      "   Lost: 108   (83.08%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 140\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (0.71%)\n",
      "   Draw: 24   (17.14%)\n",
      "   Lost: 115   (82.14%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 150\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (0.67%)\n",
      "   Draw: 26   (17.33%)\n",
      "   Lost: 123   (82.00%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 160\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (0.62%)\n",
      "   Draw: 27   (16.88%)\n",
      "   Lost: 132   (82.50%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 170\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (0.59%)\n",
      "   Draw: 33   (19.41%)\n",
      "   Lost: 136   (80.00%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 180\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (0.56%)\n",
      "   Draw: 34   (18.89%)\n",
      "   Lost: 145   (80.56%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n",
      "Episode 190\n",
      "Results for DQN Agent 0\n",
      "   Win:  1   (0.53%)\n",
      "   Draw: 34   (17.89%)\n",
      "   Lost: 155   (81.58%)\n",
      "Results for DQN Agent 1\n",
      "   Win:  0   (0.00%)\n",
      "   Draw: 0   (0.00%)\n",
      "   Lost: 0   (0.00%)\n"
     ]
    }
   ],
   "source": [
    "eval_against_mcts(env, agents, mcts_agents, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_minimax(env, trained_agents, minimax_agents, value_function, maximum_depth):\n",
    "  \"\"\"Evaluates `trained_agents` against `minimax` for.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = minimax_agents[:]\n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    time_step = env.reset()\n",
    "    episode_rewards = 0\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      if player_id == player_pos:\n",
    "        agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "        action_list = [agent_output.action]\n",
    "      else:\n",
    "        _ , action = minimax.alpha_beta_search(env.game, env.get_state, value_function=value_function, maximum_depth = maximum_depth,maximizing_player_id=player_id)\n",
    "        action_list = [action]\n",
    "      time_step = env.step(action_list)\n",
    "    sum_episode_rewards[player_pos] = time_step.rewards[player_pos]    \n",
    "  return sum_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax with Depth of:  1  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  2  Reward of DQN Agents:  [0. 1.]\n",
      "Minimax with Depth of:  3  Reward of DQN Agents:  [0. 1.]\n",
      "Minimax with Depth of:  4  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  5  Reward of DQN Agents:  [1. 0.]\n",
      "Minimax with Depth of:  6  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  7  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  8  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  9  Reward of DQN Agents:  [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    agents2 = [agents[1], agents[0]]\n",
    "    # 0 for tie, 1 for win, -1 for lost\n",
    "    for i in range(1,10):\n",
    "        print(\"Minimax with Depth of: \", i , \" Reward of DQN Agents: \", eval_against_minimax(env, agents, minimax_agents, evaluate, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
