{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta16A_T1fzFg",
    "outputId": "de787c24-cb2a-4576-b493-b3ecc310e7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 10:34:15.539016: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/titus/anaconda3/envs/tf/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import dqn\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "from open_spiel.python.algorithms import minimax\n",
    "from open_spiel.python.algorithms import minimax_agent\n",
    "from open_spiel.python.algorithms import mcts\n",
    "from open_spiel.python.algorithms import mcts_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Lm5xaFzxgSw6"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "#Directory to save/load the agent models.\n",
    "checkpoint_dir = \"/Users/titus/Desktop/MLAgents/Checkers2\"\n",
    "\n",
    "#Episode frequency at which the DQN agent models are saved\n",
    "save_every = int(1e4)\n",
    "\n",
    "#Number of training episodes.\n",
    "num_train_episodes = int(1e6)\n",
    "\n",
    "#Episode frequency at which the DQN agents are evaluated.\n",
    "eval_every = 100\n",
    "\n",
    "\n",
    "# DQN model hyper-parameters\n",
    "\n",
    "#Number of hidden units in the Q-Network MLP\n",
    "hidden_layers_sizes = [64, 64]\n",
    "\n",
    "#Size of the replay buffer\n",
    "replay_buffer_capacity = int(1e5)\n",
    "\n",
    "#Number of transitions to sample at each learning step.                     \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(state):\n",
    "  kWhite = state.observation_string().count('o')\n",
    "  kWhiteKing = state.observation_string().count('8') - 1 #one row is also labeld 8\n",
    "  kBlack = state.observation_string().count('+') \n",
    "  kBlackKing = state.observation_string().count('*')\n",
    "  return -1 * state.current_player() * (kWhite + 2*kWhiteKing - kBlack - 2*kBlackKing) #player 0 moves white pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 10:35:38.885205: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid1\n"
     ]
    }
   ],
   "source": [
    "game = \"checkers\"\n",
    "num_players = 2\n",
    "env = rl_environment.Environment(game, include_full_state=True)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "#random agents for evaluation\n",
    "minimax_agents = [\n",
    "    minimax_agent.MiniMaxAgent(env.game, env.get_state, player_id=idx, num_actions=num_actions, maximum_depth=8, value_function=evaluate)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "mcts_bot = mcts.MCTSBot(env.game, 1.5, 100, mcts.RandomRolloutEvaluator())\n",
    "mcts_agents = [\n",
    "        mcts_agent.MCTSAgent(player_id=idx, num_actions=num_actions, mcts_bot=mcts_bot)\n",
    "        for idx in range(num_players)\n",
    "]\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    agents = [\n",
    "          dqn.DQN(\n",
    "              session=sess,\n",
    "              player_id=idx,\n",
    "              state_representation_size=info_state_size,\n",
    "              num_actions=num_actions,\n",
    "              hidden_layers_sizes=hidden_layers_sizes,\n",
    "              replay_buffer_capacity=replay_buffer_capacity,\n",
    "              batch_size=batch_size) for idx in range(num_players)\n",
    "      ]\n",
    "    for agent in agents:\n",
    "        agent.restore(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_mcts(env, trained_agents, mcts_agents, num_episodes):\n",
    "  \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  results = np.zeros((num_players, 3), dtype=int)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = mcts_agents[:]    \n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    for _ in range(num_episodes):\n",
    "      if num_epsiodes % 100:\n",
    "        print(num_episodes)\n",
    "      time_step = env.reset()\n",
    "      episode_rewards = 0\n",
    "      while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "        action_list = [agent_output.action]\n",
    "        time_step = env.step(action_list)\n",
    "        episode_rewards += time_step.rewards[player_pos]\n",
    "      results[player_pos, int(episode_rewards % 3)] += 1\n",
    "      sum_episode_rewards[player_pos] += episode_rewards\n",
    "  results_percentage = (results * 100).astype(float) / num_episodes\n",
    "  results_percentage_str =  np.array([\"{:.2f}%\".format(x) for x in results_percentage.flatten()]).reshape(results_percentage.shape)\n",
    "  for player_pos in range(num_players):\n",
    "      print(f\"Results for DQN Agent {player_pos}\")\n",
    "      print(f\"   Win:  {results[player_pos, 1]}   ({results_percentage_str[player_pos, 1]})\")\n",
    "      print(f\"   Draw: {results[player_pos, 0]}   ({results_percentage_str[player_pos, 0]})\") \n",
    "      print(f\"   Lost: {results[player_pos, 2]}   ({results_percentage_str[player_pos, 2]})\")\n",
    "  return sum_episode_rewards / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_against_mcts(env, agents, mcts_agents, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_minimax(env, trained_agents, minimax_agents, value_function, maximum_depth):\n",
    "  \"\"\"Evaluates `trained_agents` against `minimax` for.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = minimax_agents[:]\n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    time_step = env.reset()\n",
    "    episode_rewards = 0\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      if player_id == player_pos:\n",
    "        agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "        action_list = [agent_output.action]\n",
    "      else:\n",
    "        _ , action = minimax.alpha_beta_search(env.game, env.get_state, value_function=value_function, maximum_depth = maximum_depth,maximizing_player_id=player_id)\n",
    "        action_list = [action]\n",
    "      time_step = env.step(action_list)\n",
    "    sum_episode_rewards[player_pos] = time_step.rewards[player_pos]    \n",
    "  return sum_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax with Depth of:  1  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  2  Reward of DQN Agents:  [0. 1.]\n",
      "Minimax with Depth of:  3  Reward of DQN Agents:  [0. 1.]\n",
      "Minimax with Depth of:  4  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  5  Reward of DQN Agents:  [1. 0.]\n",
      "Minimax with Depth of:  6  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  7  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  8  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  9  Reward of DQN Agents:  [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    agents2 = [agents[1], agents[0]]\n",
    "    # 0 for tie, 1 for win, -1 for lost\n",
    "    for i in range(1,10):\n",
    "        print(\"Minimax with Depth of: \", i , \" Reward of DQN Agents: \", eval_against_minimax(env, agents, minimax_agents, evaluate, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
