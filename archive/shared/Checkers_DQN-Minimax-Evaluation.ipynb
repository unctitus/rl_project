{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta16A_T1fzFg",
    "outputId": "de787c24-cb2a-4576-b493-b3ecc310e7d8"
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python import rl_agent\n",
    "from open_spiel.python.algorithms import minimax\n",
    "from open_spiel.python.algorithms import dqn\n",
    "from open_spiel.python.algorithms import random_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta16A_T1fzFg",
    "outputId": "de787c24-cb2a-4576-b493-b3ecc310e7d8"
   },
   "outputs": [],
   "source": [
    "class MiniMaxAgent(rl_agent.AbstractAgent):\n",
    "  \"\"\"MiniMax agent class.\"\"\"\n",
    "\n",
    "  def __init__(self, \n",
    "               game,\n",
    "               state,\n",
    "               player_id,\n",
    "               num_actions,\n",
    "               maximum_depth, \n",
    "               value_function=None,\n",
    "               name=\"minimax_agent\"):\n",
    "    assert num_actions > 0\n",
    "    self._game = game\n",
    "    self._state = state\n",
    "    self._player_id = player_id\n",
    "    self._num_actions = num_actions\n",
    "    self._maximum_depth = maximum_depth\n",
    "    self._value_function = value_function\n",
    "    \n",
    "\n",
    "  def step(self, time_step, is_evaluation=False, state=None):\n",
    "    # If it is the end of the episode, don't select an action.\n",
    "    if time_step.last():\n",
    "      return\n",
    "    if state is None:\n",
    "      state = self._game.new_initial_state()\n",
    "\n",
    "    # Pick a random legal action.\n",
    "    cur_legal_actions = time_step.observations[\"legal_actions\"][self._player_id]\n",
    "    _ , action = minimax.alpha_beta_search(self._game, \n",
    "                                           state, \n",
    "                                           value_function=self._value_function, \n",
    "                                           maximum_depth = self._maximum_depth,\n",
    "                                           maximizing_player_id=self._player_id)\n",
    "    probs = np.zeros(self._num_actions)\n",
    "    probs[cur_legal_actions] = 1.0 / len(cur_legal_actions)\n",
    "\n",
    "    return rl_agent.StepOutput(action=action, probs=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Lm5xaFzxgSw6"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "#Directory to save/load the agent models.\n",
    "checkpoint_dir = \"\"\n",
    "\n",
    "#Episode frequency at which the DQN agent models are saved\n",
    "save_every = int(1e4)\n",
    "\n",
    "#Number of training episodes.\n",
    "num_train_episodes = int(1e6)\n",
    "\n",
    "#Episode frequency at which the DQN agents are evaluated.\n",
    "eval_every = 100\n",
    "\n",
    "\n",
    "# DQN model hyper-parameters\n",
    "\n",
    "#Number of hidden units in the Q-Network MLP\n",
    "hidden_layers_sizes = [64, 64]\n",
    "\n",
    "#Size of the replay buffer\n",
    "replay_buffer_capacity = int(1e5)\n",
    "\n",
    "#Number of transitions to sample at each learning step.                     \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(state):\n",
    "  kWhite = state.observation_string().count('o')\n",
    "  kWhiteKing = state.observation_string().count('8') - 1 #one row is also labeld 8\n",
    "  kBlack = state.observation_string().count('+') \n",
    "  kBlackKing = state.observation_string().count('*')\n",
    "  return -1 * state.current_player() * (kWhite + 2*kWhiteKing - kBlack - 2*kBlackKing) #player 0 moves white pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 17:58:31.447294: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/q_network_pid1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/titus/Desktop/MLAgents/Checkers2/target_q_network_pid1\n"
     ]
    }
   ],
   "source": [
    "game = \"checkers\"\n",
    "num_players = 2\n",
    "env = rl_environment.Environment(game)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "#random agents for evaluation\n",
    "minimax_agents = [\n",
    "    MiniMaxAgent(env.game, env.get_state, player_id=idx, num_actions=num_actions, maximum_depth=8, value_function=evaluate)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "sess = tf.Session()\n",
    "\n",
    "with sess.as_default():\n",
    "    agents = [\n",
    "          dqn.DQN(\n",
    "              session=sess,\n",
    "              player_id=idx,\n",
    "              state_representation_size=info_state_size,\n",
    "              num_actions=num_actions,\n",
    "              hidden_layers_sizes=hidden_layers_sizes,\n",
    "              replay_buffer_capacity=replay_buffer_capacity,\n",
    "              batch_size=batch_size) for idx in range(num_players)\n",
    "      ]\n",
    "    for agent in agents:\n",
    "        agent.restore(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_minimax(env, trained_agents, minimax_agents, value_function, maximum_depth):\n",
    "  \"\"\"Evaluates `trained_agents` against `minimax` for.\"\"\"\n",
    "  num_players = len(trained_agents)\n",
    "  sum_episode_rewards = np.zeros(num_players)\n",
    "  for player_pos in range(num_players):\n",
    "    cur_agents = minimax_agents[:]\n",
    "    cur_agents[player_pos] = trained_agents[player_pos]\n",
    "    time_step = env.reset()\n",
    "    episode_rewards = 0\n",
    "    while not time_step.last():\n",
    "      player_id = time_step.observations[\"current_player\"]\n",
    "      if player_id == player_pos:\n",
    "        agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "        action_list = [agent_output.action]\n",
    "      else:\n",
    "        _ , action = minimax.alpha_beta_search(env.game, env.get_state, value_function=value_function, maximum_depth = maximum_depth,maximizing_player_id=player_id)\n",
    "        action_list = [action]\n",
    "      time_step = env.step(action_list)\n",
    "    sum_episode_rewards[player_pos] = time_step.rewards[player_pos] \n",
    "  return sum_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimax with Depth of:  1  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  2  Reward of DQN Agents:  [0. 1.]\n",
      "Minimax with Depth of:  3  Reward of DQN Agents:  [0. 1.]\n",
      "Minimax with Depth of:  4  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  5  Reward of DQN Agents:  [1. 0.]\n",
      "Minimax with Depth of:  6  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  7  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  8  Reward of DQN Agents:  [1. 1.]\n",
      "Minimax with Depth of:  9  Reward of DQN Agents:  [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    agents2 = [agents[1], agents[0]]\n",
    "    # 0 for tie, 1 for win, -1 for lost\n",
    "    for i in range(1,10):\n",
    "        print(\"Minimax with Depth of: \", i , \" Reward of DQN Agents: \", eval_against_minimax(env, agents, minimax_agents, evaluate, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
