{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7BOirz1fvdw",
        "outputId": "c52c7eae-b6ab-440d-a606-9360c64ed6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open_spiel\n",
            "  Downloading open_spiel-1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (23.1.2)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (23.1.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.21.5 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from open_spiel) (1.11.4)\n",
            "Collecting ml-collections>=0.1.1 (from open_spiel)\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->open_spiel) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->open_spiel) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->open_spiel) (21.6.0)\n",
            "Building wheels for collected packages: ml-collections\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94505 sha256=1a73a2ebf8cbbac7577f270f610cc07c57445d6179f253e3986d0ed9fc47550d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built ml-collections\n",
            "Installing collected packages: ml-collections, open_spiel\n",
            "Successfully installed ml-collections-0.1.1 open_spiel-1.4\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install open_spiel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta16A_T1fzFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0004e7-f37f-4036-c15f-a890e63bc239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "from absl import logging\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from open_spiel.python import rl_environment\n",
        "from open_spiel.python.algorithms import dqn\n",
        "from open_spiel.python.algorithms import random_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm5xaFzxgSw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb04769-b9a3-40bb-887f-6b435ea5c73d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import datetime\n",
        "import pytz\n",
        "import os\n",
        "# Training parameters\n",
        "\n",
        "#Directory to save/load the agent models.\n",
        "time = datetime.datetime.utcnow().astimezone(pytz.timezone('US/Eastern')).strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "checkpoint_dir = \"/content/drive/My Drive/TrainedAgents/Checkers/\" + time\n",
        "os.makedirs(checkpoint_dir)\n",
        "#Episode frequency at which the DQN agent models are saved\n",
        "save_every = int(1e4)\n",
        "\n",
        "#Number of training episodes.\n",
        "num_train_episodes = int(1e6)\n",
        "\n",
        "#Episode frequency at which the DQN agents are evaluated.\n",
        "eval_every = 1000\n",
        "\n",
        "\n",
        "# DQN model hyper-parameters\n",
        "\n",
        "#Number of hidden units in the Q-Network MLP\n",
        "hidden_layers_sizes = [64, 64]\n",
        "\n",
        "#Size of the replay buffer\n",
        "replay_buffer_capacity = int(1e5)\n",
        "\n",
        "#Number of transitions to sample at each learning step.                     )\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k64klwHugYyw"
      },
      "outputs": [],
      "source": [
        "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n",
        "  \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
        "  num_players = len(trained_agents)\n",
        "  sum_episode_rewards = np.zeros(num_players)\n",
        "  for player_pos in range(num_players):\n",
        "    cur_agents = random_agents[:]\n",
        "    cur_agents[player_pos] = trained_agents[player_pos]\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = env.reset()\n",
        "      episode_rewards = 0\n",
        "      while not time_step.last():\n",
        "        player_id = time_step.observations[\"current_player\"]\n",
        "        if env.is_turn_based:\n",
        "          agent_output = cur_agents[player_id].step(\n",
        "              time_step, is_evaluation=True)\n",
        "          action_list = [agent_output.action]\n",
        "        else:\n",
        "          agents_output = [\n",
        "              agent.step(time_step, is_evaluation=True) for agent in cur_agents\n",
        "          ]\n",
        "          action_list = [agent_output.action for agent_output in agents_output]\n",
        "        time_step = env.step(action_list)\n",
        "        episode_rewards += time_step.rewards[player_pos]\n",
        "      sum_episode_rewards[player_pos] += episode_rewards\n",
        "  return sum_episode_rewards / num_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "rRQPDuDkfwyY",
        "outputId": "51ca5e2a-06bb-4fd4-87a3-2bb753a748fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Training...\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-b10469d37165>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m       \u001b[0mplayer_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"current_player\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_turn_based\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0magent_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0maction_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_spiel/python/algorithms/dqn.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, time_step, is_evaluation, add_transition_record)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_counter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learn_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_loss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_counter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_target_network_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/open_spiel/python/algorithms/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mare_final_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_final_step\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mlegal_actions_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     loss, _ = self._session.run(\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learn_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         feed_dict={\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    973\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    974\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m           if (not is_tensor_handle_feed and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "game = \"checkers\"\n",
        "num_players = 2\n",
        "env = rl_environment.Environment(game)\n",
        "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
        "num_actions = env.action_spec()[\"num_actions\"]\n",
        "\n",
        "# random agents for evaluation\n",
        "random_agents = [\n",
        "    random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
        "    for idx in range(num_players)\n",
        "]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  hidden_layers_sizes = [int(l) for l in hidden_layers_sizes]\n",
        "  # pylint: disable=g-complex-comprehension\n",
        "  agents = [\n",
        "      dqn.DQN(\n",
        "          session=sess,\n",
        "          player_id=idx,\n",
        "          state_representation_size=info_state_size,\n",
        "          num_actions=num_actions,\n",
        "          hidden_layers_sizes=hidden_layers_sizes,\n",
        "          replay_buffer_capacity=replay_buffer_capacity,\n",
        "          batch_size=batch_size) for idx in range(num_players)\n",
        "  ]\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  result = []\n",
        "  print(\"Start Training...\")\n",
        "  for ep in range(num_train_episodes):\n",
        "    if (ep + 1) % 100 == 0:\n",
        "        print(ep+1)\n",
        "    if (ep + 1) % eval_every == 0:\n",
        "      r_mean = eval_against_random_bots(env, agents, random_agents, 1000)\n",
        "      logging.info(\"[%s] Mean episode rewards %s\", ep + 1, r_mean)\n",
        "      print(\"Mean episode rewards %s\", ep + 1, r_mean)\n",
        "      result.append(r_mean)\n",
        "    if (ep + 1) % save_every == 0:\n",
        "      for agent in agents:\n",
        "        agent.save(checkpoint_dir)\n",
        "\n",
        "    time_step = env.reset()\n",
        "    while not time_step.last():\n",
        "      player_id = time_step.observations[\"current_player\"]\n",
        "      if env.is_turn_based:\n",
        "        agent_output = agents[player_id].step(time_step)\n",
        "        action_list = [agent_output.action]\n",
        "      else:\n",
        "        agents_output = [agent.step(time_step) for agent in agents]\n",
        "        action_list = [agent_output.action for agent_output in agents_output]\n",
        "      time_step = env.step(action_list)\n",
        "\n",
        "    # Episode is over, step all agents with final info state.\n",
        "    for agent in agents:\n",
        "      agent.step(time_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "k-sat7hSZo-i",
        "outputId": "76a3663b-be5c-4be9-dd8b-91d1d0018f6f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-719e0a42d26c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpt_r_mean0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpt_r_mean1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ep = [x for x in range(len(result))]\n",
        "pt_r_mean0 = [y[0] for y in result]\n",
        "pt_r_mean1 = [y[1] for y in result]\n",
        "\n",
        "plt.plot(ep,pt_r_mean0, c='red')\n",
        "plt.plot(ep,pt_r_mean1, c='blue')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Mean episode rewards')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "game = \"checkers\"\n",
        "num_players = 2\n",
        "env = rl_environment.Environment(game)\n",
        "print(env.game)\n",
        "print(env.get_state)\n",
        "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
        "num_actions = env.action_spec()[\"num_actions\"]\n",
        "\n",
        "# random agents for evaluation\n",
        "#minimax_agents = [\n",
        "#    minimax_agent.MiniMaxAgent(env.game, env.get_state, player_id=idx, num_actions=num_actions, maximum_depth=8, value_function=evaluate)\n",
        "#    for idx in range(num_players)\n",
        "#]\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    agents = [\n",
        "          dqn.DQN(\n",
        "              session=sess,\n",
        "              player_id=idx,\n",
        "              state_representation_size=info_state_size,\n",
        "              num_actions=num_actions,\n",
        "              hidden_layers_sizes=hidden_layers_sizes,\n",
        "              replay_buffer_capacity=replay_buffer_capacity,\n",
        "              batch_size=batch_size) for idx in range(num_players)\n",
        "      ]\n",
        "    print(type(agents[0]))\n",
        "    agents[0].restore(checkpoint_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkmwZ5urYzPM",
        "outputId": "ada08546-4656-4792-db8f-0ff59878739c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkers()\n",
            "None\n",
            "<class 'open_spiel.python.algorithms.dqn.DQN'>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}